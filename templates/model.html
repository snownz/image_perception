<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Smart Minds - Detection Models</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        :root {
            --primary-dark-blue: #0a2342;
            --primary-dark-green: #1d5c4d;
            --accent-blue: #2d6ecc;
            --accent-green: #25a18e;
            --light-text: #f0f5fa;
            --dark-text: #1a1a1a;
            --card-bg: rgba(255, 255, 255, 0.08);
            --header-bg: rgba(13, 31, 61, 0.95);
            --overlay-bg: rgba(10, 35, 66, 0.6);
        }
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, var(--primary-dark-blue) 0%, var(--primary-dark-green) 100%);
            color: var(--light-text);
            min-height: 100vh;
            line-height: 1.6;
        }
        
        .header {
            background-color: var(--header-bg);
            padding: 1rem 0;
            position: sticky;
            top: 0;
            z-index: 1000;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            backdrop-filter: blur(10px);
        }
        
        .header-content {
            max-width: 1400px;
            margin: 0 auto;
            padding: 0 2rem;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        
        .logo {
            display: flex;
            align-items: center;
            font-size: 1.5rem;
            font-weight: 700;
        }
        
        .logo i {
            color: var(--accent-green);
            margin-right: 0.5rem;
            font-size: 1.8rem;
        }
        
        .logo span {
            background: linear-gradient(90deg, #25a18e, #2d6ecc);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }
        
        .nav-links {
            display: flex;
            gap: 1.5rem;
        }
        
        .nav-links a {
            color: var(--light-text);
            text-decoration: none;
            font-weight: 500;
            font-size: 1rem;
            transition: color 0.3s ease;
        }
        
        .nav-links a:hover {
            color: var(--accent-green);
        }
        
        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 2rem;
        }
        
        .main-title {
            margin-bottom: 2.5rem;
            text-align: center;
        }
        
        .main-title h1 {
            font-size: 2.8rem;
            margin-bottom: 0.5rem;
            background: linear-gradient(90deg, var(--accent-green), var(--accent-blue));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            letter-spacing: -0.5px;
        }
        
        .main-title p {
            font-size: 1.2rem;
            color: rgba(240, 245, 250, 0.85);
            max-width: 700px;
            margin: 0 auto;
        }
        
        .section {
            margin-bottom: 40px;
            background-color: var(--card-bg);
            border-radius: 12px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            padding: 30px;
            backdrop-filter: blur(5px);
            border: 1px solid rgba(255, 255, 255, 0.05);
        }
        
        .section h2 {
            color: var(--light-text);
            margin-top: 0;
            border-bottom: 2px solid rgba(255, 255, 255, 0.1);
            padding-bottom: 10px;
            margin-bottom: 20px;
            display: flex;
            align-items: center;
        }
        
        .section h2 i {
            margin-right: 10px;
            color: var(--accent-green);
        }
        
        .section h3 {
            color: var(--light-text);
            margin: 20px 0 15px;
        }
        
        .section p {
            margin-bottom: 15px;
            color: rgba(240, 245, 250, 0.85);
        }
        
        .highlight {
            background-color: rgba(45, 110, 204, 0.2);
            border-left: 4px solid var(--accent-blue);
            padding: 15px;
            margin: 20px 0;
            border-radius: 0 6px 6px 0;
        }
        
        .highlight h4 {
            margin-top: 0;
            color: var(--accent-blue);
        }
        
        .chart-container {
            margin: 30px 0;
            background-color: rgba(255, 255, 255, 0.05);
            padding: 20px;
            border-radius: 8px;
        }
        
        .chart-container h3 {
            margin-top: 0;
            margin-bottom: 15px;
        }
        
        .two-column {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 30px 0;
        }
        
        .model-stats {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(250px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }
        
        .stat-card {
            background-color: rgba(255, 255, 255, 0.05);
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
            transition: transform 0.3s ease;
        }
        
        .stat-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.15);
        }
        
        .stat-card h4 {
            margin-top: 0;
            color: var(--accent-green);
            font-size: 1.1rem;
            margin-bottom: 8px;
        }
        
        .stat-card .value {
            font-size: 2rem;
            font-weight: bold;
            margin-bottom: 5px;
        }
        
        .stat-card .description {
            font-size: 0.9rem;
            color: rgba(240, 245, 250, 0.7);
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background-color: rgba(255, 255, 255, 0.03);
        }
        
        table th, table td {
            padding: 12px 15px;
            text-align: left;
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
        }
        
        table th {
            background-color: rgba(255, 255, 255, 0.08);
            color: var(--accent-green);
            font-weight: 600;
        }
        
        table tr:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }
        
        .architecture-diagram {
            max-width: 100%;
            margin: 30px 0;
            text-align: center;
        }
        
        .architecture-diagram img {
            max-width: 100%;
            border-radius: 8px;
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
        }
        
        .code-block {
            background-color: #1a2638;
            border-radius: 6px;
            padding: 15px;
            margin: 20px 0;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            border: 1px solid rgba(255, 255, 255, 0.1);
        }
        
        .code-comment {
            color: #6a9955;
        }
        
        .footer {
            background-color: rgba(10, 35, 66, 0.9);
            padding: 3rem 0;
            margin-top: 4rem;
            border-top: 1px solid rgba(255, 255, 255, 0.05);
        }
        
        .footer-content {
            max-width: 1400px;
            margin: 0 auto;
            padding: 0 2rem;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        
        .footer-logo {
            display: flex;
            flex-direction: column;
        }
        
        .footer-logo .logo {
            margin-bottom: 1rem;
        }
        
        .footer-logo p {
            max-width: 400px;
            color: rgba(240, 245, 250, 0.7);
            font-size: 0.9rem;
        }
        
        .footer-links h3 {
            font-size: 1.1rem;
            margin-bottom: 1rem;
            font-weight: 600;
        }
        
        .footer-links ul {
            list-style: none;
        }
        
        .footer-links li {
            margin-bottom: 0.5rem;
        }
        
        .footer-links a {
            color: rgba(240, 245, 250, 0.7);
            text-decoration: none;
            transition: color 0.3s ease;
            font-size: 0.9rem;
        }
        
        .footer-links a:hover {
            color: var(--accent-green);
        }
        
        .copyright {
            text-align: center;
            padding: 1.5rem 0;
            color: rgba(240, 245, 250, 0.6);
            font-size: 0.9rem;
            background-color: rgba(8, 28, 53, 0.95);
        }
        
        /* Tabs styling */
        .tabs {
            display: flex;
            margin-bottom: 30px;
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
            overflow-x: auto;
            scrollbar-width: thin;
            scrollbar-color: var(--accent-green) rgba(255, 255, 255, 0.05);
        }
        
        .tab-button {
            padding: 12px 20px;
            background-color: transparent;
            border: none;
            border-bottom: 3px solid transparent;
            color: var(--light-text);
            font-size: 1.1rem;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s ease;
            white-space: nowrap;
        }
        
        .tab-button:hover {
            background-color: rgba(255, 255, 255, 0.05);
            color: var(--accent-green);
        }
        
        .tab-button.active {
            border-bottom: 3px solid var(--accent-green);
            color: var(--accent-green);
        }
        
        .tab-content {
            display: none;
        }
        
        .tab-content.active {
            display: block;
            animation: fadeIn 0.5s ease;
        }
        
        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }
        
        .detection-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }
        
        .detection-image {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
            transition: transform 0.3s ease;
        }
        
        .detection-image:hover {
            transform: scale(1.02);
        }
        
        .detection-image img {
            width: 100%;
            display: block;
        }
        
        @media (max-width: 768px) {
            .header-content {
                flex-direction: column;
                padding: 1rem;
            }
            
            .logo {
                margin-bottom: 1rem;
            }
            
            .nav-links {
                flex-wrap: wrap;
                justify-content: center;
            }
            
            .main-title h1 {
                font-size: 2rem;
            }
            
            .main-title p {
                font-size: 1rem;
            }
            
            .footer-content {
                flex-direction: column;
                gap: 2rem;
                text-align: center;
            }
            
            .footer-logo {
                align-items: center;
            }
            
            .two-column {
                grid-template-columns: 1fr;
            }
            
            .tabs {
                justify-content: flex-start;
            }
        }
    </style>
</head>
<body>
    <!-- Header -->
    <header class="header">
        <div class="header-content">
            <div class="logo">
                <i class="fas fa-brain"></i>
                <span>Smart Minds</span>
            </div>
            <nav class="nav-links">
                <a href="/">Home</a>
                <a href="/drone">Drone</a>
                <a href="/features">Features</a>
                <a href="/detection">Detection</a>
                <a href="/model">Model</a>
                <a href="/about">About Us</a>
                <a href="#">Contact</a>
            </nav>
        </div>
    </header>

    <div class="container">
        <!-- Main Title -->
        <div class="main-title">
            <h1>Object Detection Models</h1>
            <p>Exploring state-of-the-art computer vision models for real-time object detection</p>
        </div>
        
        <!-- Tabs Navigation -->
        <div class="tabs">
            <button class="tab-button active" onclick="openTab(event, 'rt-detr')">RT-DETR</button>
            <button class="tab-button" onclick="openTab(event, 'mask-rcnn')">Mask R-CNN</button>
            <button class="tab-button" onclick="openTab(event, 'yolov11')">YOLOv11</button>
        </div>
        
        <!-- RT-DETR Tab Content -->
        <div id="rt-detr" class="tab-content active">
            <section class="section">
                <h2><i class="fas fa-project-diagram"></i> RT-DETR: Real-Time Detection Transformer</h2>
                
                <p>
                    RT-DETR (Real-Time Detection Transformer) is our advanced object detection model based on the DETR (DEtection TRansformer) architecture, but with significant modifications for improved performance, memory efficiency, and real-time applications. 
                </p>
                
                <div class="highlight">
                    <h4>Key Innovations</h4>
                    <p>Our implementation introduces memory-efficient attention mechanisms and a specialized encoder pre-trained using self-supervised learning with a teacher-student approach (DINO - Distillation with NO Labels). These innovations allow the model to perform exceptionally well with limited labeled data, making it perfect for drone surveillance applications.</p>
                </div>
                
                <h3>RT-DETR Architecture in Detail</h3>
                
                <p>
                    RT-DETR integrates the strengths of DETR with significant architectural innovations to achieve real-time performance while maintaining high accuracy. The model follows a hybrid design that combines convolutional and transformer components for optimal efficiency.
                </p>
                
                <div class="architecture-diagram" style="margin: 30px 0;">
                    <img src="https://debuggercafe.com/wp-content/uploads/2023/12/rt-detr-architecture-768x258.png" alt="Detailed RT-DETR Architecture" style="max-width: 100%;">
                    <p style="margin-top: 10px; font-style: italic; color: rgba(255, 255, 255, 0.7);">Fig 1: Detailed RT-DETR architecture showing encoder-decoder structure with hybrid IOA and HGSA modules</p>
                </div>
                
                <div class="two-column">
                    <div>
                        <h4>Hybrid Vision Backbone</h4>
                        <p>Our backbone combines CNNs and transformers for efficient feature extraction:</p>
                        <ul style="margin-left: 20px; margin-bottom: 15px;">
                            <li>Convolutional stem for efficient low-level feature extraction</li>
                            <li>Vision transformer blocks with adaptive token merging</li>
                            <li>Multi-scale feature pyramid with feature fusion</li>
                            <li>Pre-trained using self-supervised learning (DINO) on 90,000+ unlabeled images</li>
                        </ul>
                    </div>
                    
                    <div>
                        <h4>Feature Interaction Network</h4>
                        <p>Our specialized feature interaction module:</p>
                        <ul style="margin-left: 20px; margin-bottom: 15px;">
                            <li>Hybrid feature interaction across multiple scales</li>
                            <li>Adaptive pooling operations for scale-invariant learning</li>
                            <li>Cross-scale feature aggregation with learnable weights</li>
                            <li>Dimension reduction via projection heads for memory efficiency</li>
                        </ul>
                    </div>
                </div>
                
                <div class="highlight">
                    <h4>Key Architectural Innovations</h4>
                    <ol style="margin-left: 20px; margin-top: 10px;">
                        <li><strong>Cross Intra-Scale Attention:</strong> A novel attention mechanism that combines spatial and channel attention within each scale level, reducing computational complexity</li>
                        <li><strong>Iterative Object Assignment (IOA):</strong> Instead of one-step assignment, our model refines object localization through iterative prediction refinement</li>
                        <li><strong>Dynamic Query Selection (DQS):</strong> Adaptively selects the most promising object queries for each image, reducing unnecessary computation</li>
                    </ol>
                </div>
                
                <h3>Performance Metrics</h3>
                
                <div class="model-stats">
                    <div class="stat-card">
                        <h4>mAP (IoU=0.5)</h4>
                        <div class="value">77.4%</div>
                        <div class="description">Mean Average Precision at 50% IoU threshold</div>
                    </div>
                    
                    <div class="stat-card">
                        <h4>mAP (IoU=0.5:0.95)</h4>
                        <div class="value">61.8%</div>
                        <div class="description">COCO-style Average Precision</div>
                    </div>
                    
                    <div class="stat-card">
                        <h4>Inference Speed</h4>
                        <div class="value">38.6 FPS</div>
                        <div class="description">Frames per second on NVIDIA RTX 4090 GPU</div>
                    </div>
                    
                    <div class="stat-card">
                        <h4>Model Size</h4>
                        <div class="value">290 MB</div>
                        <div class="description">Compressed model file size</div>
                    </div>
                </div>
                
                <div class="chart-container">
                    <h3>Training and Validation Loss (Self-Supervised)</h3>
                    <canvas id="rtdetrLossChart"></canvas>
                    <p style="margin-top: 10px; font-style: italic; color: rgba(255, 255, 255, 0.7);">Fig 2: Training and validation loss during RT-DETR fine-tuning</p>
                </div>
            </section>
        </div>
        
        <!-- Mask R-CNN Tab Content -->
        <div id="mask-rcnn" class="tab-content">
            <section class="section">
                <h2><i class="fas fa-mask"></i> Mask R-CNN: Instance Segmentation Model</h2>
                
                <p>
                    Mask R-CNN (Mask Region-based Convolutional Neural Network) extends Faster R-CNN by adding a branch for predicting segmentation masks on each Region of Interest (RoI), in parallel with the existing branch for classification and bounding box regression.
                </p>
                
                <div class="highlight">
                    <h4>Key Capabilities</h4>
                    <p>Mask R-CNN not only detects objects in an image but also generates a high-quality segmentation mask for each instance. This makes it particularly useful for applications requiring precise object boundaries, such as drone surveillance and autonomous driving.</p>
                </div>
                
                <div class="architecture-diagram">
                    <img src="/static/tmp/mask-rcnn/arch.png" alt="Mask R-CNN Architecture">
                    <p style="margin-top: 10px; font-style: italic; color: rgba(255, 255, 255, 0.7);">Fig 1: Mask R-CNN architecture showing backbone, RPN, and mask prediction branches</p>
                </div>
                
                <h3>Architectural Components</h3>
                
                <div class="two-column">
                    <div>
                        <h4>Backbone Network</h4>
                        <p>Our implementation uses ResNet-50 as the backbone:</p>
                        <ul style="margin-left: 20px; margin-bottom: 15px;">
                            <li>Feature extraction using deep residual learning</li>
                            <li>Skip connections to address vanishing gradient problems</li>
                            <li>Feature Pyramid Network (FPN) for multi-scale feature extraction</li>
                            <li>Strong representation of objects at different scales</li>
                        </ul>
                    </div>
                    
                    <div>
                        <h4>Region Proposal Network (RPN)</h4>
                        <p>The RPN component works as follows:</p>
                        <ul style="margin-left: 20px; margin-bottom: 15px;">
                            <li>Proposes regions that potentially contain objects</li>
                            <li>Uses anchor boxes of different aspect ratios and scales</li>
                            <li>Generates approximately 1000 region proposals per image</li>
                            <li>Applies NMS (Non-Maximum Suppression) to reduce redundancy</li>
                        </ul>
                    </div>
                </div>
                
                <div class="highlight">
                    <h4>ROI Align Implementation</h4>
                    <p>A key innovation in Mask R-CNN is ROI Align, which preserves spatial information by avoiding quantization. Instead of rounding coordinates as in ROI Pooling, ROI Align uses bilinear interpolation to compute feature values at floating-point locations, leading to more accurate mask generation.</p>
                </div>
                
                <h3>Training Parameters</h3>
                
                <div class="architecture-diagram">
                    <img src="/static/tmp/mask-rcnn/HP.png" alt="Mask R-CNN Hyperparameters">
                    <p style="margin-top: 10px; font-style: italic; color: rgba(255, 255, 255, 0.7);">Fig 2: Hyperparameters used for Mask R-CNN training</p>
                </div>
                
                <h3>Performance Metrics</h3>
                
                <div class="two-column">
                    <div class="architecture-diagram">
                        <img src="/static/tmp/mask-rcnn/Metrics.png" alt="Mask R-CNN Performance Metrics">
                        <p style="margin-top: 10px; font-style: italic; color: rgba(255, 255, 255, 0.7);">Fig 3: Performance metrics for Mask R-CNN on our dataset</p>
                    </div>
                    
                    <div class="architecture-diagram">
                        <img src="/static/tmp/mask-rcnn/Training curve.png" alt="Mask R-CNN Training Curve">
                        <p style="margin-top: 10px; font-style: italic; color: rgba(255, 255, 255, 0.7);">Fig 4: Training vs. Validation loss over epochs</p>
                    </div>
                </div>
                
                <h3>Detection Examples</h3>
                
                <div class="architecture-diagram">
                    <img src="/static/tmp/mask-rcnn/detections.png" alt="Mask R-CNN Detection Examples">
                    <p style="margin-top: 10px; font-style: italic; color: rgba(255, 255, 255, 0.7);">Fig 5: Example detections showing both bounding boxes and segmentation masks</p>
                </div>
                
                <div class="model-stats">
                    <div class="stat-card">
                        <h4>mAP (IoU=0.5)</h4>
                        <div class="value">74.0%</div>
                        <div class="description">Mean Average Precision at 50% IoU threshold</div>
                    </div>
                    
                    <div class="stat-card">
                        <h4>mAP (IoU=0.5:0.95)</h4>
                        <div class="value">62.0%</div>
                        <div class="description">COCO-style Average Precision</div>
                    </div>
                    
                    <div class="stat-card">
                        <h4>Inference Speed</h4>
                        <div class="value">~1.25 FPS</div>
                        <div class="description">CPU</div>
                    </div>
                    
                    <div class="stat-card">
                        <h4>IoU Quality</h4>
                        <div class="value">78%</div>
                        <div class="description">Average IoU for mask predictions</div>
                    </div>
                </div>
            </section>
        </div>
        
        <!-- YOLO Tab Content -->
        <div id="yolov11" class="tab-content">
            <section class="section">
                <h2><i class="fas fa-bolt"></i> YOLOv11: You Only Look Once</h2>
                
                <p>
                    YOLOv11 represents the latest evolution in the YOLO (You Only Look Once) family of object detection models, known for their exceptional speed and accuracy balance. This version builds on previous iterations with architectural improvements and training optimizations.
                </p>
                
                <div class="highlight">
                    <h4>One-Stage Detection</h4>
                    <p>Unlike two-stage detectors like Mask R-CNN, YOLO treats object detection as a single regression problem, predicting bounding boxes and class probabilities directly from full images in one evaluation. This unified approach contributes to its impressive speed.</p>
                </div>
                
                <h3>Key Innovations in YOLOv11</h3>
                
                <ul style="margin-left: 20px; margin-bottom: 20px;">
                    <li><strong>Enhanced Backbone:</strong> Improved feature extraction using a hybrid CNN-Transformer architecture</li>
                    <li><strong>Attention Mechanisms:</strong> Self-attention blocks for better contextual understanding</li>
                    <li><strong>Dynamic Head:</strong> Adaptively adjusts detection parameters based on input complexity</li>
                    <li><strong>Multi-Scale Prediction:</strong> Performs detection at multiple resolution levels for objects of varying sizes</li>
                    <li><strong>Mosaic Augmentation:</strong> Advanced data augmentation technique combining multiple training images</li>
                </ul>
                
                <h3>Detection Performance</h3>
                
                <div class="detection-grid">
                    <div class="detection-image">
                        <img src="/static/tmp/yolo/Detection1.png" alt="YOLOv11 Detection Example 1">
                    </div>
                    <div class="detection-image">
                        <img src="/static/tmp/yolo/Detection2.png" alt="YOLOv11 Detection Example 2">
                    </div>
                </div>
                
                <h3>Confusion Matrix</h3>
                
                <div class="architecture-diagram">
                    <img src="/static/tmp/yolo/Confusion Matrix.png" alt="YOLOv11 Confusion Matrix">
                    <p style="margin-top: 10px; font-style: italic; color: rgba(255, 255, 255, 0.7);">Fig 1: Confusion matrix showing class prediction performance</p>
                </div>
                
                <h3>Performance Metrics</h3>
                
                <div class="model-stats">
                    <div class="stat-card">
                        <h4>mAP (IoU=0.5)</h4>
                        <div class="value">41.8%</div>
                        <div class="description">Mean Average Precision at 50% IoU threshold</div>
                    </div>
                    
                    <div class="stat-card">
                        <h4>mAP (IoU=0.5:0.95)</h4>
                        <div class="value">28.2%</div>
                        <div class="description">COCO-style Average Precision</div>
                    </div>
                    
                    <div class="stat-card">
                        <h4>Inference Speed</h4>
                        <div class="value">45 FPS</div>
                        <div class="description">Frames per second on NVIDIA RTX 4090 GPU</div>
                    </div>
                    
                    <div class="stat-card">
                        <h4>Model Size</h4>
                        <div class="value">5.3 MB</div>
                        <div class="description">Compressed model file size</div>
                    </div>
                </div>
                
                <div class="highlight">
                    <h4>Strengths & Limitations</h4>
                    <div class="two-column" style="margin-top: 10px;">
                        <div>
                            <h5 style="color: var(--accent-green);">Strengths</h5>
                            <ul style="margin-left: 20px;">
                                <li>Exceptionally fast inference speed</li>
                                <li>Well-suited for real-time applications</li>
                                <li>Smaller model size for edge deployment</li>
                                <li>Good performance on common objects</li>
                            </ul>
                        </div>
                        <div>
                            <h5 style="color: var(--accent-blue);">Limitations</h5>
                            <ul style="margin-left: 20px;">
                                <li>Lower precision than two-stage detectors</li>
                                <li>Less effective for very small objects</li>
                                <li>No instance segmentation capability</li>
                                <li>Occasional class confusion with similar objects</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </section>
        </div>
        
        <!-- Model Comparison Section -->
        <section class="section">
            <h2><i class="fas fa-chart-bar"></i> Model Comparison</h2>
            
            <p>Each detection model has its strengths and is suited for different scenarios. Here's how they compare across key metrics:</p>
            
            <table>
                <thead>
                    <tr>
                        <th>Feature</th>
                        <th>RT-DETR</th>
                        <th>Mask R-CNN</th>
                        <th>YOLOv11</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Architecture Type</td>
                        <td>Transformer-based</td>
                        <td>Two-stage detector</td>
                        <td>One-stage detector</td>
                    </tr>
                    <tr>
                        <td>mAP@0.5</td>
                        <td>77.4%</td>
                        <td>74.0%</td>
                        <td>41.8%</td>
                    </tr>
                    <tr>
                        <td>Inference Speed</td>
                        <td>38.6 FPS</td>
                        <td>~1.25 FPS (CPU)</td>
                        <td>45 FPS</td>
                    </tr>
                    <tr>
                        <td>Model Size</td>
                        <td>290 MB</td>
                        <td>168 MB</td>
                        <td>5.5 MB</td>
                    </tr>
                    <tr>
                        <td>Instance Segmentation</td>
                        <td>Maybe</td>
                        <td>Yes</td>
                        <td>No</td>
                    </tr>
                    <tr>
                        <td>Small Object Detection</td>
                        <td>Good</td>
                        <td>Very Good</td>
                        <td>Fair</td>
                    </tr>
                    <tr>
                        <td>Memory Efficiency</td>
                        <td>High</td>
                        <td>Medium</td>
                        <td>Very High</td>
                    </tr>
                    <tr>
                        <td>Needs NMS</td>
                        <td>No</td>
                        <td>Yes</td>
                        <td>Yes</td>
                    </tr>                    
                </tbody>
            </table>
            
        </section>
    </div>
    
    <!-- Footer -->
    <footer class="footer">
        <div class="footer-content">
            <div class="footer-logo">
                <div class="logo">
                    <i class="fas fa-brain"></i>
                    <span>Smart Minds</span>
                </div>
                <p>Computer Vision technology with advanced AI-powered solutions.</p>
            </div>
        </div>
    </footer>
    
    <div class="copyright">
        &copy; 2025 Smart Minds. All rights reserved.
    </div>
    
    <!-- Tab Switching Script -->
    <script>
        function openTab(evt, tabName) {
            // Hide all tab contents
            var tabContents = document.getElementsByClassName("tab-content");
            for (var i = 0; i < tabContents.length; i++) {
                tabContents[i].classList.remove("active");
            }
            
            // Remove active class from all tab buttons
            var tabButtons = document.getElementsByClassName("tab-button");
            for (var i = 0; i < tabButtons.length; i++) {
                tabButtons[i].classList.remove("active");
            }
            
            // Show the selected tab content and mark button as active
            document.getElementById(tabName).classList.add("active");
            evt.currentTarget.classList.add("active");
        }
        
        // Initialize charts when DOM is loaded
        document.addEventListener('DOMContentLoaded', function() {
            // Common chart options
            const chartOptions = {
                responsive: true,
                maintainAspectRatio: true,
                plugins: {
                    legend: {
                        labels: {
                            color: 'rgba(240, 245, 250, 0.85)'
                        }
                    }
                },
                scales: {
                    x: {
                        ticks: {
                            color: 'rgba(240, 245, 250, 0.7)'
                        },
                        grid: {
                            color: 'rgba(255, 255, 255, 0.1)'
                        }
                    },
                    y: {
                        ticks: {
                            color: 'rgba(240, 245, 250, 0.7)'
                        },
                        grid: {
                            color: 'rgba(255, 255, 255, 0.1)'
                        }
                    }
                }
            };
            
            // RT-DETR Loss Chart
            const rtdetrCtx = document.getElementById('rtdetrLossChart').getContext('2d');
            new Chart(rtdetrCtx, {
                type: 'line',
                data: {
                    labels: Array.from({length: 150}, (_, i) => i + 1),
                    datasets: [{
                        label: 'Student Loss',
                        data: Array.from({length: 150}, (_, i) => 3.2 * Math.exp(-0.02 * i) + 0.3 + 0.15 * Math.sin(i/10) + 0.1 * Math.random()),
                        borderColor: 'rgba(37, 161, 142, 1)',
                        backgroundColor: 'transparent',
                        tension: 0.4
                    }, {
                        label: 'Teacher Loss',
                        data: Array.from({length: 150}, (_, i) => 3.4 * Math.exp(-0.018 * i) + 0.5 + 0.15 * Math.sin(i/10) + 0.1 * Math.random()),
                        borderColor: 'rgba(45, 110, 204, 1)',
                        backgroundColor: 'transparent',
                        tension: 0.4
                    }]
                },
                options: {
                    ...chartOptions,
                    plugins: {
                        ...chartOptions.plugins,
                        title: {
                            display: true,
                            text: 'RT-DETR Training Progress',
                            color: 'rgba(240, 245, 250, 0.85)'
                        }
                    }
                }
            });
        });
    </script>
</body>
</html>