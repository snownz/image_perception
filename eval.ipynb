{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import ObjectDetectionModel\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_label = {\n",
    "    0: \"person\",\n",
    "    1: \"birds\",\n",
    "    2: \"parking meter\",\n",
    "    3: \"stop sign\",\n",
    "    4: \"street sign\",\n",
    "    5: \"fire hydrant\",\n",
    "    6: \"traffic light\",\n",
    "    7: \"motorcycle\",\n",
    "    8: \"bicycle\",\n",
    "    9: \"LMVs\",\n",
    "    10: \"HMVs\",\n",
    "    11: \"animals\",\n",
    "    12: \"poles\",\n",
    "    13: \"barricades\",\n",
    "    14: \"traffic cones\",\n",
    "    15: \"mailboxes\",\n",
    "    16: \"stones\",\n",
    "    17: \"small walls\",\n",
    "    18: \"bins\",\n",
    "    19: \"furniture\",\n",
    "    20: \"pot plant\",\n",
    "    21: \"sign boards\",\n",
    "    22: \"boxes\",\n",
    "    23: \"trees\",\n",
    "}\n",
    "\n",
    "# Generate 24 distinct colors\n",
    "int_to_color = {i: (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)) for i in range(24)}\n",
    "det_to_color = {i: (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)) for i in range(300)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def resize_image(image_tensor, size):\n",
    "    return F.interpolate( image_tensor.unsqueeze(0), size = ( size, size ),\n",
    "                          mode = 'bilinear', align_corners = False ).squeeze(0)\n",
    "\n",
    "@dataclass\n",
    "class ConfigBackbone:\n",
    "    in_channels: float = 3\n",
    "    embed_dim: float = 512\n",
    "    num_heads: float = 8\n",
    "    depth: float = 4\n",
    "    num_tokens: float = 4096\n",
    "    model = ''\n",
    "\n",
    "@dataclass\n",
    "class BackboneConfig:\n",
    "    in_channels: float = 3\n",
    "    embed_dim: float = 384\n",
    "    num_heads: float = 8\n",
    "    depth: float = 4\n",
    "    num_tokens: float = 4096\n",
    "    model: str = \"linear\"\n",
    "\n",
    "@dataclass\n",
    "class ComputePrecision:\n",
    "    grad_scaler: bool = True\n",
    "\n",
    "@dataclass\n",
    "class HungarianLoss:\n",
    "    lambda_bbox: int = 1.0\n",
    "    lambda_cls: float = 1.0\n",
    "    image_width: float = 512\n",
    "    image_height: float = 512\n",
    "    num_classes: int = 25\n",
    "\n",
    "@dataclass\n",
    "class Detection:\n",
    "    nc: int = 24\n",
    "    ch: tuple = (384, 384, 384)\n",
    "    hd: int = 256  # hidden dim\n",
    "    nq: int = 300  # num queries\n",
    "    ndp: int = 4  # num decoder points\n",
    "    nh: int = 8  # num head\n",
    "    ndl: int = 6  # num decoder layers\n",
    "    d_ffn: int = 1024  # dim of feedforward\n",
    "    dropout: float = 0.0\n",
    "    act: nn.Module = nn.ReLU()\n",
    "    eval_idx: int = -1\n",
    "    # Training args\n",
    "    learnt_init_query: bool = False\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    log_dir: str = \"./models/\"\n",
    "    name: str = \"detection_v5_small\"\n",
    "    backbone_name: str = \"encoder_v5\"\n",
    "    compute_precision: ComputePrecision = field( default_factory = ComputePrecision )\n",
    "    backbone: BackboneConfig = field( default_factory = BackboneConfig )\n",
    "    hungarian_loss: HungarianLoss = field( default_factory = HungarianLoss )\n",
    "    detection: Detection = field( default_factory = Detection )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()\n",
    "device = ( 'cuda' if torch.cuda.is_available() else 'cpu' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ObjectDetectionModel( cfg, device )\n",
    "# model.load( chpt = 150000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to( device )\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook variables\n",
    "hook_outputs = {}\n",
    "\n",
    "# Hook functions\n",
    "def hook_fn(capture):\n",
    "    def execute_hook(module, input, output):\n",
    "        global hook_outputs\n",
    "        hook_outputs[capture] = {\n",
    "            'input': input,\n",
    "            'output': output\n",
    "        }\n",
    "    return execute_hook\n",
    "\n",
    "# Register the hook\n",
    "model.detection.enc_score_head.register_forward_hook( hook_fn( 'proposal_queries' ) ) # Proposal Queries ( Tokens )\n",
    "model.backbone.backbone.net.model.layers[0].register_forward_hook( hook_fn( 'features' ) ) # Backbone\n",
    "model.backbone.backbone.net.model.layers[31].register_forward_hook( hook_fn( 'features_large_objects' ) ) # Backbone\n",
    "model.backbone.backbone.net.model.layers[28].register_forward_hook( hook_fn( 'features_medium_objects' ) ) # Backbone\n",
    "model.backbone.backbone.net.model.layers[25].register_forward_hook( hook_fn( 'features_small_objects' ) ) # Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to extract the informations form the hook\n",
    "\n",
    "def occurrence_indices(strings):\n",
    "    counts = {}\n",
    "    indices = []\n",
    "    for s in strings:\n",
    "        # Get the current count (default is 0)\n",
    "        count = counts.get(s, 0)\n",
    "        indices.append(count)\n",
    "        # Update the count for this string\n",
    "        counts[s] = count + 1\n",
    "    return indices\n",
    "\n",
    "def process_detections_tokens(shapes):\n",
    "    \n",
    "    global hook_outputs\n",
    "\n",
    "    # shapes = [ ( 90, 160 ), ( 45, 80 ), ( 22, 40 ) ]\n",
    "\n",
    "    # Get the outputs from the hook\n",
    "    enc_outputs_scores = hook_outputs['proposal_queries']['output'].to('cpu')  # [bs, h*w, num_classes]\n",
    "\n",
    "    # Get shape information\n",
    "    bs, hw_total, num_classes = enc_outputs_scores.shape\n",
    "\n",
    "    # Apply softmax to the scores\n",
    "    enc_outputs_scores = enc_outputs_scores.sigmoid()\n",
    "\n",
    "    # Get the max scores and labels\n",
    "    scores, _ = enc_outputs_scores.max( -1 )\n",
    "\n",
    "    # get top-k tokens and values from the all 3 feature maps\n",
    "    topk_values, topk_indices = torch.topk( scores, 300, dim = 1 )\n",
    "    topk_indices = topk_indices[0]  # shape: (300,)\n",
    "    topk_values  = topk_values[0]   # shape: (300,)\n",
    "    indices_idx = torch.arange( 0, 300 )  # shape: (300,)\n",
    "\n",
    "    level0_end = shapes[0][0] * shapes[0][1]\n",
    "    level1_end = level0_end + shapes[1][0] * shapes[1][1]\n",
    "    level2_end = level1_end + shapes[2][0] * shapes[2][1]  # Should equal hw_total\n",
    "\n",
    "    # Isolate tokens for level 0: indices in [0, level0_end)\n",
    "    mask0 = ( topk_indices < level0_end )\n",
    "    top_indices_feat_0 = topk_indices[mask0]\n",
    "    top_values_feat_0  = topk_values[mask0]\n",
    "    indices_idx_feat_0 = indices_idx[mask0]\n",
    "\n",
    "    # Isolate tokens for level 1: indices in [level0_end, level1_end)\n",
    "    mask1 = ( topk_indices >= level0_end ) & ( topk_indices < level1_end )\n",
    "    top_indices_feat_1 = topk_indices[mask1] - level0_end\n",
    "    top_values_feat_1  = topk_values[mask1]\n",
    "    indices_idx_feat_1 = indices_idx[mask1]\n",
    "\n",
    "    # Isolate tokens for level 2: indices in [level1_end, level2_end)\n",
    "    mask2 = ( topk_indices >= level1_end ) & ( topk_indices < level2_end )\n",
    "    top_indices_feat_2 = topk_indices[mask2] - level1_end  # local index if desired\n",
    "    top_values_feat_2  = topk_values[mask2]\n",
    "    indices_idx_feat_2 = indices_idx[mask2]\n",
    "\n",
    "    all_top_indices = [ top_indices_feat_0, top_indices_feat_1, top_indices_feat_2 ]\n",
    "    all_top_values = [ top_values_feat_0, top_values_feat_1, top_values_feat_2 ]\n",
    "    all_top_indices_idx = [ indices_idx_feat_0, indices_idx_feat_1, indices_idx_feat_2 ]\n",
    "\n",
    "    return all_top_indices, all_top_values, all_top_indices_idx\n",
    "\n",
    "def extrcat_features(feats):\n",
    "    img = feats['output'][0] + feats['output'][0].std( 0 )\n",
    "    return {\n",
    "        'image': img.cpu().numpy(), \n",
    "        'layers': len( img )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize the features\n",
    "\n",
    "def get_proposals_heatmap_overlay(image, labels, indices, detectors, int_to_label, process_detections_tokens, det_to_color):\n",
    "    \"\"\"\n",
    "    Creates an object proposals heatmap overlay with detector annotations.\n",
    "    \n",
    "    Parameters:\n",
    "        image (np.array): The original image (RGB).\n",
    "        labels (iterable): Labels used to create the legend.\n",
    "        indices (iterable): Indices corresponding to each label.\n",
    "        detectors (iterable): Detector IDs.\n",
    "        int_to_label (dict or callable): Mapping from integer label to text label.\n",
    "        process_detections_tokens (callable): Function that processes detections tokens;\n",
    "                                                should return (all_top_indices, all_top_values, all_top_indices_idx)\n",
    "        det_to_color (dict): Mapping from detector id to a color tuple (R, G, B) in 0-255 range.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: The final blended RGB image with the heatmap and detector annotations.\n",
    "    \"\"\"\n",
    "    # Create the legend mapping for detectors.\n",
    "    legend = {d: int_to_label[l.item()] + '_' + str(i) for l, i, d in zip(labels, indices, detectors)}\n",
    "    \n",
    "    # Define scales for multi-scale detection.\n",
    "    shapes = [(80, 80), (40, 40), (20, 20)]\n",
    "    \n",
    "    # Process detection tokens to get indices, values, and indices positions for each scale.\n",
    "    all_top_indices, all_top_values, all_top_indices_idx = process_detections_tokens(shapes)\n",
    "    \n",
    "    original_h, original_w = image.shape[0:2]\n",
    "    heatmap_overlay = np.zeros((original_h, original_w), dtype=np.float32)\n",
    "    all_detector_coords = []\n",
    "    \n",
    "    # Loop through each scale and update the heatmap overlay.\n",
    "    for i, ((h, w), s_indices, values, dxs) in enumerate(zip(shapes, all_top_indices, all_top_values, all_top_indices_idx)):\n",
    "        scale_y, scale_x = original_h / h, original_w / w\n",
    "        circle_radius = max(1, int(min(scale_y, scale_x) // 2))\n",
    "        for idx, val, dx in zip(s_indices, values, dxs):\n",
    "            idx = idx.item()\n",
    "            feat_y, feat_x = idx // w, idx % w\n",
    "            \n",
    "            orig_y = int((feat_y + 0.5) * scale_y)\n",
    "            orig_x = int((feat_x + 0.5) * scale_x)\n",
    "            \n",
    "            orig_y = min(max(0, orig_y), original_h - 1)\n",
    "            orig_x = min(max(0, orig_x), original_w - 1)\n",
    "            \n",
    "            score_val = val.item()\n",
    "            \n",
    "            cv2.circle(heatmap_overlay, (orig_x, orig_y), circle_radius, score_val, -1)\n",
    "            \n",
    "            if dx in detectors:\n",
    "                all_detector_coords.append((orig_x, orig_y, circle_radius * 3, dx.item()))\n",
    "    \n",
    "    # Normalize the heatmap overlay.\n",
    "    if heatmap_overlay.max() > 0:\n",
    "        heatmap_overlay = heatmap_overlay / heatmap_overlay.max()\n",
    "    \n",
    "    # Create the colored heatmap using the COLORMAP_HOT.\n",
    "    heatmap_color = cv2.applyColorMap((heatmap_overlay * 255).astype(np.uint8), cv2.COLORMAP_HOT)\n",
    "    heatmap_color = cv2.cvtColor(heatmap_color, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Blend the original image with the heatmap overlay.\n",
    "    image_rgb = image.copy()\n",
    "    alpha = 0.7\n",
    "    blended = cv2.addWeighted(image_rgb, 1 - alpha, heatmap_color, alpha, 0)\n",
    "    \n",
    "    # Add 40px padding to the blended image.\n",
    "    blended = cv2.copyMakeBorder(blended, 40, 40, 40, 40, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "    \n",
    "    # Draw detector circles and text labels on the final image using OpenCV.\n",
    "    for (cx, cy, radius, n) in all_detector_coords:\n",
    "        cx += 40  # adjust for the padding\n",
    "        cy += 40  # adjust for the padding\n",
    "        # Draw circle (using the color from det_to_color, assumed to be in (R, G, B)).\n",
    "        cv2.circle(blended, (cx, cy), radius, det_to_color[n], thickness=2)\n",
    "        # Draw text label near the circle.\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(blended, legend[n], (cx + 10, cy - 15), font, 0.4, (255, 255, 255), thickness=1, lineType=cv2.LINE_AA)\n",
    "    \n",
    "    return blended\n",
    "\n",
    "def interactive_channel_viewer(image, layers):\n",
    "    \n",
    "    # Create the initial heatmap using the first channel.\n",
    "    fig = go.Figure(\n",
    "        data=go.Heatmap(\n",
    "            z=image[0],\n",
    "            colorscale='gray',\n",
    "            zmin=float(np.min(image)),\n",
    "            zmax=float(np.max(image)),\n",
    "            colorbar=dict(title=\"Intensity\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Create a slider with steps for each channel.\n",
    "    steps = []\n",
    "    for i in range(layers):\n",
    "        step = dict(\n",
    "            method=\"update\",\n",
    "            args=[{\"z\": [image[i]]}],  # update the heatmap's data\n",
    "            label=f\"Channel {i}\"\n",
    "        )\n",
    "        steps.append(step)\n",
    "    \n",
    "    sliders = [dict(\n",
    "        active=0,\n",
    "        currentvalue={\"prefix\": \"Channel: \"},\n",
    "        pad={\"t\": 50},\n",
    "        steps=steps\n",
    "    )]\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Interactive Channel Viewer\",\n",
    "        sliders=sliders,\n",
    "        xaxis=dict(title=\"Width\"),\n",
    "        yaxis=dict(title=\"Height\")\n",
    "    )\n",
    "    \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "image = cv2.imread( 'test_images/8.png' )\n",
    "image = cv2.cvtColor( image, cv2.COLOR_BGR2RGB )\n",
    "torch_image = torch.from_numpy( image ).permute( 2, 0, 1 ).float() / 255.0\n",
    "torch_image = torch_image.to( device )\n",
    "\n",
    "# resize the image to 640x640\n",
    "torch_image = resize_image( torch_image, 640 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes, scores, labels, detectors = model.predict( torch_image, stride_slices = 32, confidence_threshold = 0.3, iou_threshold = 0.3 )\n",
    "detectors = detectors.cpu().numpy()\n",
    "indices = occurrence_indices( labels.cpu().numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_outputs['features']['output'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = interactive_channel_viewer( **extrcat_features( hook_outputs['features'] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw, ImageFont\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "image_height, image_width = image.shape[0:2]\n",
    "image_ = ToPILImage()( image )\n",
    "draw = ImageDraw.Draw( image_ )\n",
    "font = ImageFont.load_default()\n",
    "\n",
    "for d in range( boxes.shape[0] ):\n",
    "\n",
    "    bbox = boxes[d]\n",
    "    score = scores[d]\n",
    "    label = labels[d]\n",
    "    detector = detectors[d]\n",
    "\n",
    "    x_center, y_center, width, height = bbox\n",
    "    x_min = ( x_center - width / 2 ) * image_width\n",
    "    y_min = ( y_center - height / 2 ) * image_height\n",
    "    x_max = ( x_center + width / 2 ) * image_width\n",
    "    y_max = ( y_center + height / 2 ) * image_height\n",
    "    box = np.array( [ x_min, y_min, x_max, y_max ] )\n",
    "    \n",
    "    label_name = int_to_label[label.item()]\n",
    "    score = score.item() * 100\n",
    "    detector_number = detector.item()\n",
    "    draw.rectangle( box, outline = int_to_color[label.item()], width = 2 )\n",
    "    draw.text( ( x_min, y_min ), f\"{label_name} \\n {score:.2f}% \\n {detector_number}\", fill = \"red\", font = font )\n",
    "\n",
    "image_.show()\n",
    "image_.save( \"test_images/8_p.png\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = get_proposals_heatmap_overlay( image, labels, indices, detectors, int_to_label, process_detections_tokens, det_to_color )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the image\n",
    "plt.imshow( image )\n",
    "plt.axis( 'off' )\n",
    "plt.title( 'Proposals Heatmap Overlay' )\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
