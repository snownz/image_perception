{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from src.models import PerceptionModel\n",
    "from src.data import build_data_loader\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import HDBSCAN, KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image_tensor, size):\n",
    "    return F.interpolate( image_tensor.unsqueeze(0), size = ( size, size ),\n",
    "                          mode = 'bilinear', align_corners = False ).squeeze(0)\n",
    "\n",
    "def pad_image(image_tensor, target_size=64):\n",
    "    \"\"\"\n",
    "    Pads the input image tensor with a black border so that the final image\n",
    "    has dimensions (target_size, target_size). The image is centered.\n",
    "    \n",
    "    Assumes image_tensor is of shape [C, H, W].\n",
    "    \n",
    "    Parameters:\n",
    "    - image_tensor: torch.Tensor of shape [C, H, W]\n",
    "    - target_size: Desired output size (both height and width)\n",
    "    \n",
    "    Returns:\n",
    "    - padded_image: torch.Tensor of shape [C, target_size, target_size]\n",
    "    \"\"\"\n",
    "    # Get original image dimensions\n",
    "    _, H, W = image_tensor.shape\n",
    "    \n",
    "    # Compute the padding sizes for height and width\n",
    "    pad_height = target_size - H\n",
    "    pad_width = target_size - W\n",
    "    \n",
    "    # Ensure that the image is smaller than the target size\n",
    "    if pad_height < 0 or pad_width < 0:\n",
    "        raise ValueError(\"The image dimensions are larger than the target size.\")\n",
    "    \n",
    "    # Calculate padding for each side (left, right, top, bottom)\n",
    "    pad_left = pad_width // 2\n",
    "    pad_right = pad_width - pad_left\n",
    "    pad_top = pad_height // 2\n",
    "    pad_bottom = pad_height - pad_top\n",
    "    \n",
    "    # F.pad expects the padding tuple in the order: (pad_left, pad_right, pad_top, pad_bottom)\n",
    "    padded_image = F.pad(image_tensor, (pad_left, pad_right, pad_top, pad_bottom), mode='constant', value=0)\n",
    "    return padded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BackboneConfig:\n",
    "    in_channels: float = 3\n",
    "    embed_dim: float = 384\n",
    "    num_heads: float = 8\n",
    "    depth: float = 4\n",
    "    num_tokens: float = 4096\n",
    "    model: str = \"linear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PerceptionModel( vars( BackboneConfig() ), \"./models/\", 'encoder_v1', device ).to(device)\n",
    "model.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image = cv2.imread( 'front_1706545937.png' )\n",
    "image = cv2.cvtColor( image, cv2.COLOR_BGR2RGB )\n",
    "torch_image = torch.from_numpy( image ).permute( 2, 0, 1 ).float() / 255.0\n",
    "torch_image = torch_image.unsqueeze( 0 ).to( device )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = None\n",
    "def hook_fn(module, input, output):\n",
    "    global feature_maps\n",
    "    # feature_maps = input[0].detach().cpu()\n",
    "    feature_maps = output.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backbone.net.model.layers[15].register_forward_hook( hook_fn )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model( torch_image )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_index = 3\n",
    "t_ = feature_maps[0].permute( 1, 2, 0 )\n",
    "plt.figure( figsize = ( 8, 8 ) )\n",
    "plt.imshow( t_[:,:,layer_index], cmap = 'magma' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader = build_data_loader( 'dataset', batch_size = 4, global_crops_size = 640 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_iter = iter(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# denormalize = lambda x: 0.5 * x + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     with torch.amp.autocast( enabled = True, device_type = \"cuda\", dtype = torch.bfloat16 ):\n",
    "#         data = next(data_iter)\n",
    "#         images = data['collated_global_crops'].cuda( non_blocking = True )\n",
    "#         tokens = model( images )\n",
    "\n",
    "# tokens = [ \n",
    "#     tokens[0].view( 8, 64, 64, -1 ).to(torch.float32),\n",
    "#     tokens[1].view( 8, 32, 32, -1 ).to(torch.float32),\n",
    "#     tokens[2].view( 8, 16, 16, -1 ).to(torch.float32),\n",
    "# ]\n",
    "\n",
    "# # plot the imgae\n",
    "# grid = make_grid( images, nrow = 4, normalize = True ).cpu()\n",
    "# # grid = make_grid( denormalize( images ), nrow = 4, normalize = True ).cpu()\n",
    "\n",
    "# plt.figure( figsize = ( 10, 10 ) )\n",
    "# plt.imshow( grid.permute(1, 2, 0) )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 2\n",
    "# layer_index = 70\n",
    "# t_ = [ resize_image( x[index][:,:,layer_index][None], 80 ) for x in tokens ]\n",
    "# t_ = [ ( x - x.mean() ) / ( x.std() + 1e-6 ) for x in t_ ]\n",
    "# grid = make_grid( t_, nrow = 3 )\n",
    "# plt.figure( figsize = ( 12, 12 ) )\n",
    "# plt.imshow( grid.cpu().permute(1, 2, 0)[:,:,0], cmap = 'magma' )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = tokens[0][index].permute(2, 0, 1).cpu().numpy()\n",
    "\n",
    "# channels, s, _ = image.shape\n",
    "\n",
    "# # Reshape the image to have one pixel per row (each pixel is a vector of length 'channels')\n",
    "# pixels = image.reshape(channels, s * s).T  # shape: [s*s, channels]\n",
    "\n",
    "# # Define the number of clusters (for example, 3)\n",
    "# n_clusters = 20\n",
    "# random_colors = np.random.rand(n_clusters, 3)\n",
    "\n",
    "# # Initialize and fit the GMM\n",
    "# # cluster = GaussianMixture(n_components=n_clusters, random_state=0)\n",
    "# # cluster = HDBSCAN( min_cluster_size = 16, min_samples = 16 )\n",
    "# cluster = KMeans( n_clusters = n_clusters )\n",
    "# labels = cluster.fit_predict( pixels )\n",
    "\n",
    "# # Reshape the labels back to the original spatial dimensions\n",
    "# clustered_image = labels.reshape(s, s)\n",
    "# clustered_image = random_colors[clustered_image].transpose(2, 0, 1)\n",
    "# clustered_image = torch.tensor( clustered_image ).to(torch.float32).to(device)\n",
    "\n",
    "# image = make_grid( \n",
    "#     [ \n",
    "#         resize_image( clustered_image, 640 ), \n",
    "#         images[index], \n",
    "#         resize_image( resize_image( images[index], s ), 640 ) \n",
    "#     ] \n",
    "# ).cpu().numpy()\n",
    "\n",
    "# # Plot the original image\n",
    "# plt.figure(figsize=(18, 12))\n",
    "# plt.imshow(image.transpose(1, 2, 0))\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
