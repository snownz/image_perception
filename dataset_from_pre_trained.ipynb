{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import build_data_loader_train_detection, denormalize_transform\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "from ultralytics import RTDETR\n",
    "import cv2\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "data_loader = build_data_loader_train_detection( 'dataset_detection', batch_size = 4, max_objects = 200, max_poly_points = 64, crop_size = 512, mode = 'seg' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_FOLDER = \"dataset_detection_pre\"\n",
    "OUTPUT_IMAGE_FOLDER = 'images/train'\n",
    "OUTPUT_ANNOTATED_IMAGE_FOLDER = 'annotated_images/train'\n",
    "OUTPUT_LABEL_FOLDER = 'labels/train'\n",
    "TRAIN_TXT_PATH = 'train.txt'\n",
    "DATA_YAML_PATH = 'data.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs( os.path.join( BASE_FOLDER, OUTPUT_IMAGE_FOLDER ), exist_ok = True )\n",
    "os.makedirs( os.path.join( BASE_FOLDER, OUTPUT_ANNOTATED_IMAGE_FOLDER ), exist_ok = True )\n",
    "os.makedirs( os.path.join( BASE_FOLDER, OUTPUT_LABEL_FOLDER ), exist_ok = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = YOLO( 'yolo/yolo11x.pt', task = 'detect' )\n",
    "y12 = YOLO( 'yolo/yolo12x.pt', task = 'detect' )\n",
    "y11 = YOLO( 'yolo/yolo11x.pt', task = 'detect' )\n",
    "dterr = RTDETR( \"yolo/rtdetr-x.pt\" )\n",
    "to_pil = T.ToPILImage()\n",
    "train_txt_lines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_to_custom = {\n",
    "    0: 1,    # person ➜ Person\n",
    "    1: 9,    # bicycle ➜ Bicycle\n",
    "    2: 10,   # car ➜ LMVs\n",
    "    3: 8,    # motorcycle ➜ Motorcycle\n",
    "    5: 11,   # bus ➜ HMVs\n",
    "    6: 11,   # train ➜ HMVs\n",
    "    7: 11,   # truck ➜ HMVs\n",
    "    9: 7,    # traffic light ➜ Traffic Light\n",
    "    10: 6,   # fire hydrant ➜ Fire Hydrant\n",
    "    11: 4,   # stop sign ➜ Stop Sign\n",
    "    12: 3,   # parking meter ➜ Parking Meter\n",
    "    14: 2,   # bird ➜ Birds\n",
    "    16: 12,  # dog ➜ Animals\n",
    "    17: 12,  # horse ➜ Animals\n",
    "    18: 12,  # sheep ➜ Animals\n",
    "    19: 12,  # cow ➜ Animals\n",
    "    20: 12,  # elephant ➜ Animals\n",
    "    21: 12,  # bear ➜ Animals\n",
    "    22: 12,  # zebra ➜ Animals\n",
    "    23: 12,  # giraffe ➜ Animals\n",
    "    56: 20,  # chair ➜ Furniture\n",
    "    58: 21,  # potted plant ➜ Pot Plant\n",
    "    13: 20,  # bench ➜ Furniture\n",
    "    33: 22,  # kite ➜ Sign Boards (loosely)\n",
    "    45: 20,  # bowl ➜ Furniture\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou_topleft(box1, box2):\n",
    "    \"\"\"\n",
    "    Compute IoU for two boxes in [x_left, y_top, w, h] (normalized) format.\n",
    "    \"\"\"\n",
    "    x1, y1, w1, h1 = box1\n",
    "    x2, y2, w2, h2 = box2\n",
    "\n",
    "    box1_x1, box1_y1 = x1, y1\n",
    "    box1_x2, box1_y2 = x1 + w1, y1 + h1\n",
    "\n",
    "    box2_x1, box2_y1 = x2, y2\n",
    "    box2_x2, box2_y2 = x2 + w2, y2 + h2\n",
    "\n",
    "    inter_x1 = max(box1_x1, box2_x1)\n",
    "    inter_y1 = max(box1_y1, box2_y1)\n",
    "    inter_x2 = min(box1_x2, box2_x2)\n",
    "    inter_y2 = min(box1_y2, box2_y2)\n",
    "\n",
    "    inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)\n",
    "    area1 = w1 * h1\n",
    "    area2 = w2 * h2\n",
    "    union = area1 + area2 - inter_area\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    return inter_area / union\n",
    "\n",
    "def center_to_topleft(box_center):\n",
    "    \"\"\"\n",
    "    Convert a box from center format [cx, cy, w, h] to top-left format [x_left, y_top, w, h].\n",
    "    \"\"\"\n",
    "    cx, cy, w, h = box_center\n",
    "    return [cx - w/2, cy - h/2, w, h]\n",
    "\n",
    "def deduplicate_detections(detections, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Remove duplicate detections from a list by keeping the larger detection.\n",
    "    \n",
    "    detections: list of tuples (custom_class, box)\n",
    "      where box is in [x_left, y_top, w, h] (normalized) format.\n",
    "    iou_threshold: if IoU between two boxes of the same class exceeds this threshold,\n",
    "                   they are considered duplicates.\n",
    "                   \n",
    "    Returns:\n",
    "      A deduplicated list of detections, keeping the detection with the larger area\n",
    "      for overlapping detections.\n",
    "    \"\"\"\n",
    "    deduped = []\n",
    "    for det in detections:\n",
    "        cls_det, box_det = det\n",
    "        area_det = box_det[2] * box_det[3]  # area = w * h\n",
    "        duplicate_found = False\n",
    "        for idx, d in enumerate(deduped):\n",
    "            cls_existing, box_existing = d\n",
    "            if cls_existing == cls_det:\n",
    "                iou = compute_iou_topleft(box_det, box_existing)\n",
    "                if iou > iou_threshold:\n",
    "                    area_existing = box_existing[2] * box_existing[3]\n",
    "                    # If new detection has a bigger area, replace the existing one.\n",
    "                    if area_det > area_existing:\n",
    "                        deduped[idx] = det\n",
    "                    duplicate_found = True\n",
    "                    break\n",
    "        if not duplicate_found:\n",
    "            deduped.append(det)\n",
    "    return deduped\n",
    "\n",
    "def merge_detections(result, ds_labels, ds_boxes, yolo_to_custom, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Merges dataset detections with YOLO detections for one image.\n",
    "\n",
    "    - result: YOLO detection result for one image (each box has .cls and .xywhn)\n",
    "    - ds_labels: tensor of shape [N] containing dataset detection labels (0 means no detection)\n",
    "    - ds_boxes: tensor of shape [N, 4] containing dataset boxes in [x_left, y_top, w, h] normalized format\n",
    "    - yolo_to_custom: dictionary mapping YOLO class ids to your custom class ids\n",
    "    - iou_threshold: IoU threshold for matching\n",
    "\n",
    "    For detections with the same label and IoU > iou_threshold, only the detection with the larger area is kept.\n",
    "\n",
    "    Returns:\n",
    "      A list of tuples (custom_class, box) where box is in [x_left, y_top, w, h] normalized format.\n",
    "      Dataset detections are kept, and a YOLO detection is added only if it does not conflict\n",
    "      with an existing dataset detection of the same class. In case of conflict, the bigger detection is kept.\n",
    "    \"\"\"\n",
    "    final_detections = []\n",
    "    \n",
    "    # 1. Add dataset detections (non-zero labels) as they are.\n",
    "    for label, box in zip(ds_labels, ds_boxes):\n",
    "        if label.item() != 0:\n",
    "            final_detections.append( ( int( label.item() ) - 1, box.tolist() ) )\n",
    "    \n",
    "    # 2. Process YOLO detections.\n",
    "    for yolo_box in result.boxes:\n",
    "        yolo_cls = int( yolo_box.cls )\n",
    "        if yolo_cls not in yolo_to_custom:\n",
    "            continue\n",
    "        custom_cls = yolo_to_custom[yolo_cls] - 1\n",
    "        # Get YOLO detection in center format and convert to top-left format.\n",
    "        yolo_center = yolo_box.xywhn[0].tolist()  # [cx, cy, w, h]\n",
    "        yolo_tl = center_to_topleft(yolo_center)   # now in [x_left, y_top, w, h]\n",
    "        area_new = yolo_tl[2] * yolo_tl[3]  # area of new detection\n",
    "        confidence = yolo_box.conf[0].item()\n",
    "        if confidence < 0.5:\n",
    "            continue\n",
    "        \n",
    "        conflict_found = False\n",
    "        indices_to_remove = []\n",
    "        # Check for conflict with any existing detection of the same class.\n",
    "        for idx, (existing_cls, existing_box) in enumerate(final_detections):            \n",
    "            iou = compute_iou_topleft( yolo_tl, existing_box )\n",
    "            if iou > iou_threshold and confidence < 0.7:\n",
    "                conflict_found = True\n",
    "                break\n",
    "            if existing_cls == custom_cls:\n",
    "                if iou > iou_threshold:\n",
    "                    area_existing = existing_box[2] * existing_box[3]\n",
    "                    if area_new > area_existing:\n",
    "                        # New detection is bigger, mark the existing one for removal.\n",
    "                        indices_to_remove.append(idx)\n",
    "                    else:\n",
    "                        # Existing detection is bigger; skip adding the new one.\n",
    "                        conflict_found = True\n",
    "                        break\n",
    "        if not conflict_found:\n",
    "            # Remove any conflicting detections that are smaller.\n",
    "            for idx in sorted(indices_to_remove, reverse=True):\n",
    "                del final_detections[idx]\n",
    "            final_detections.append((custom_cls, yolo_tl))\n",
    "    \n",
    "    return final_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@8.146] global loadsave.cpp:848 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999 of 10000\r"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "max_images = 10000\n",
    "\n",
    "for batch in data_loader:\n",
    "    # Convert tensor images to NumPy arrays (and multiply by 255 if your transform normalizes to [0,1])\n",
    "    images = list(batch[0].unbind(0))\n",
    "    images = [denormalize_transform(img).permute(1, 2, 0).cpu().numpy() * 255 for img in images]\n",
    "    \n",
    "    # Get dataset detections:\n",
    "    # ds_labels: tensor of shape [batch_size, N] with labels (in [x_left, y_top, w, h] format for boxes)\n",
    "    # ds_boxes: tensor of shape [batch_size, N, 4] with boxes in [x_left, y_top, w, h] normalized format\n",
    "    ds_labels = list( batch[1].unbind(0) )\n",
    "    ds_boxes = list( batch[2].unbind(0) )\n",
    "    \n",
    "    r12 = y12.predict( images, verbose = False, conf = 0.25, iou = 0.45, agnostic_nms = True )\n",
    "    r11 = y11.predict( images, verbose = False, conf = 0.25, iou = 0.45, agnostic_nms = True )\n",
    "    rtdetr = dterr.predict( images, verbose = False )\n",
    "\n",
    "    for idx in range(len(images)):\n",
    "\n",
    "        final_detections_1 = merge_detections( rtdetr[idx], ds_labels[idx], ds_boxes[idx], yolo_to_custom, iou_threshold = 0.3 )\n",
    "        final_detections_2 = merge_detections( r12[idx], ds_labels[idx], ds_boxes[idx], yolo_to_custom, iou_threshold = 0.3 )\n",
    "        final_detections_3 = merge_detections( r11[idx], ds_labels[idx], ds_boxes[idx], yolo_to_custom, iou_threshold = 0.3 )\n",
    "\n",
    "        # Combine detections from all three sources\n",
    "        combined_detections = final_detections_1 + final_detections_2 + final_detections_3\n",
    "\n",
    "        # Deduplicate overlapping detections of the same class\n",
    "        final_detections = deduplicate_detections( combined_detections, iou_threshold = 0.1 )\n",
    "\n",
    "        # Skip sample if no final detections.\n",
    "        if not final_detections:\n",
    "            continue\n",
    "\n",
    "        filename = f'image_{i}.png'\n",
    "        output_annotated_img_path = os.path.join(BASE_FOLDER, OUTPUT_ANNOTATED_IMAGE_FOLDER, filename)\n",
    "        output_img_path = os.path.join(BASE_FOLDER, OUTPUT_IMAGE_FOLDER, filename)\n",
    "        \n",
    "        image = images[idx]\n",
    "        annotated_image = image.copy()\n",
    "        h_img, w_img, _ = annotated_image.shape\n",
    "        \n",
    "        # Draw each final detection on the image.\n",
    "        for custom_cls, box in final_detections:\n",
    "            # box is in [x_left, y_top, w, h] (normalized).\n",
    "            # Convert normalized coordinates to absolute pixel values.\n",
    "            x_left = int(box[0] * w_img)\n",
    "            y_top = int(box[1] * h_img)\n",
    "            box_w = int(box[2] * w_img)\n",
    "            box_h = int(box[3] * h_img)\n",
    "            x2 = x_left + box_w\n",
    "            y2 = y_top + box_h\n",
    "\n",
    "            if custom_cls > 23:\n",
    "                a = 10\n",
    "            \n",
    "            cv2.rectangle(annotated_image, (x_left, y_top), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(annotated_image, str(custom_cls), (x_left, y_top - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # Save annotated image.\n",
    "        cv2.imwrite(output_annotated_img_path, annotated_image)\n",
    "        cv2.imwrite(output_img_path, cv2.cvtColor( image, cv2.COLOR_BGR2RGB ) )\n",
    "        \n",
    "        # Save label file in YOLO format but with your box format ([x_left, y_top, w, h]).\n",
    "        label_path = os.path.join(BASE_FOLDER, OUTPUT_LABEL_FOLDER, Path(filename).stem + '.txt')\n",
    "        with open(label_path, 'w') as f:\n",
    "            for custom_cls, box in final_detections:\n",
    "                f.write(f\"{custom_cls} {box[0]:.6f} {box[1]:.6f} {box[2]:.6f} {box[3]:.6f}\\n\")\n",
    "        \n",
    "        train_txt_lines.append(str(Path(output_annotated_img_path).resolve()))\n",
    "        with open( os.path.join( BASE_FOLDER, TRAIN_TXT_PATH ), 'a' ) as f:\n",
    "            f.write( str(Path(output_annotated_img_path).resolve()) + '\\n' )\n",
    "        print(f\"{i} of {max_images}\", end='\\r')\n",
    "\n",
    "        del annotated_image        \n",
    "        del final_detections\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        i += 1\n",
    "        if i >= max_images:\n",
    "            break\n",
    "    \n",
    "    del rtdetr\n",
    "    del r12\n",
    "    del r11\n",
    "    del images\n",
    "    del ds_labels\n",
    "    del ds_boxes\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if i >= max_images:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train.txt\n",
    "# with open( os.path.join( BASE_FOLDER, TRAIN_TXT_PATH ), 'w' ) as f:\n",
    "#    f.write('\\n'.join(train_txt_lines))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
